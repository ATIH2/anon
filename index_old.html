<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>
<link href="https://fonts.googleapis.com/css2?family=Open+Sans&display=swap"
      rel="stylesheet">
<link rel="stylesheet" type="text/css" href="./resources/style.css" media="screen"/>

<html lang="en">
<head>
  	<title> Novel Object Synthesis via Adaptive Text-Image Harmony</title>
      <!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/
          if you update and want to force Facebook to re-scrape. -->
  	<meta property="og:image" content="./resources/fig_0.png"/>
  	<meta property="og:title" content="Novel Object Synthesis via Adaptive Text-Image Harmony" />
  	<!-- <meta property="og:description" content="A simple yet effective sampling method without any training to generate new and meaningful combinations from two given object texts in text-to-image synthesis." /> -->
    <!-- Twitter automatically scrapes this. Go to https://cards-dev.twitter.com/validator?
        if you update and want to force Twitter to re-scrape. -->
    <meta property="twitter:card"          content="summary" />
    <meta property="twitter:title"         content="Novel Object Synthesis via Adaptive Text-Image Harmony" />
    <!-- <meta property="twitter:description"   content="A simple yet effective sampling method without any training to generate new and meaningful combinations from two given object texts in text-to-image synthesis." /> -->
    <meta property="twitter:image"         content="./resources/fig_0.png" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <!-- Add your Google Analytics tag here -->
    <script async
            src="https://www.googletagmanager.com/gtag/js?id=UA-97476543-1"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() {
            dataLayer.push(arguments);
        }
        gtag('js', new Date());
        gtag('config', 'UA-97476543-1');
    </script>
    <style>
      .video-container {
        display: flex;
        flex-direction: column;
        align-items: center;
      }
      .video-row {
        display: flex;
        justify-content: space-around;
        width: 100%;
        margin-bottom: 20px;
      }
      .video-row video {
        width: 45%;
        max-width: 600px;
      }
    </style>
</head>

<body>
<div class="container">
    <div class="title">
        Novel Object Synthesis via Adaptive Text-Image Harmony
    </div>
    <!-- video   -->
    <div class="video-container">
      <div class="video-row">
        <video controls>
          <source src="resources\video\watermarked_video0c79a238db7fe42d981e8085e6a60c2ad.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
        <video controls>
          <source src="E:\paperproject\index_project\anon\resources\video\watermarked_video0cf0eb6b4098d4d888bb9b8f40d948846.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </div>
      <div class="video-row">
        <video controls>
          <source src="E:\paperproject\index_project\anon\resources\video\watermarked_video0ea5582edbc7a41b5847e107e66ccf712.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
        <video controls>
          <source src="E:\paperproject\index_project\anon\resources\video\watermarked_video0406e120ed1be4e069c111fd783e74de8.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </div>
    </div>
    <!-- video   -->
    <!-- <div class="venue">
        In ECCV 2024
    </div> -->

    <br><br>

    <div class="author">
        <a>Anonymous authors</a>
    </div>

    <br><br>
    <br><br>
    
    <img style="width: 80%;" src="./resources/first_fig.png" alt="Teaser figure."/>
    <br><br>

   

    <br>
    <p style="width: 80%; text-align: justify;font-size: 16px;font-family: 'Times New Roman', Times, serif;">
        We propose a straightforward yet powerful approach for generating novel and meaningful
        combinations from a given object text-image pair in text-to-image synthesis. Our algorithm produces
        these combined object images using the central image and its surrounding text inputs, such as Bottle
        (image) and Porcupine (text) in the left picture. Specifically, the left central image depicts a real-world
        bottle, while the right central image, representing a dog, is generated using SDXL Turbo.
    </p>
    <br>
    <hr>

    <h1>Abstract</h1>
    <p style="width: 80%; text-align: justify;">
        In this paper, we study an object synthesis task that combines an object text with
        an object image to create a new object image. However, most diffusion models
        struggle with this task due to the imbalance between text and image inputs. To
        address this issue, we propose a simple yet effective method called Adaptive Text-
        Image Harmony (ATIH) for generating novel and surprising objects. First, we
        introduce a scale factor and an injection step to balance text and image features in
        cross-attention and to preserve image information in self-attention during the text8
        image inversion diffusion process, respectively. Second, to adaptively adjust these
        parameters, we present a novel similarity score function that not only maximizes
        the similarities between the generated object image and the input text/image but
        also balances these similarities to harmonize text and image integration. Third,
        we optimize the noise to simultaneously enhance object editability and improve
        the reconstruction quality of the original object image. Extensive experiments
        demonstrate the effectiveness of our approach, showcasing remarkable object
        creations such as Bottle-Sea Lion and Dog-Lobster in Fig. 1. Project Page.
    </p>

    <br>
    <hr>
   


    <h1>Our framework</h1>
    <p style="text-align: justify">
        Framework of the combinational object synthesis incorporating a scale factor α, an
        injection step i and noise ϵt in the diffusion process. The noise ϵt is regularized to simultaneously
        enhance object editability and maintain object fidelity. Using the optimal noise ϵt, we introduce an
        adaptive harmony mechanism to adjust α and i, balancing text and image similarities.
    </p>


    <img style="width: 100%;" src="./resources/framework.png" alt="Teaser figure."/>
    <br><br>
    <hr>
    
    <h1>Comparing Results</h1>
        <!-- <p style="text-align: center">
            We conduct visual comparisons to assess the generation of combinatorial objects. In this analysis, we evaluate our BASS method against state-of-the-art T2I models, including Stable-Diffusion2<a href="#ref2"><sup>[2]</sup></a> , DALLE2<a href="#ref1"><sup>[1]</sup></a>, ERNIE-ViLG2(Baidu)<a href="#ref3"><sup>[3]</sup></a> , and Bing(Microsoft). 
        </p>
            <p style="text-align: center">
                We compare our BASS results to other T2I models with the general prompt 'Hybrid of [p1] and [p2]'.
            </p> -->
        

        <p style="text-align: justify">
            Comparisons with different image editing methods. We observe that Infedit [61]
            Masactrl [6] and InstructPix2Pix [5] struggle to fuse object images and texts, while our method
            successfully implements new object synthesis, such as fawn-bowling ball in the second row.
        </p>


        
        <img style="width: 80%;" src="./resources/compare_edit.png" alt="Teaser figure."/>

        <p style="text-align: justify">
            Comparisons with different creative mixing methods. We observe that our results
            surpass those of MagicMix [29]. For ConceptLab [16], we exclusively examine its fusion results
            without making good or bad comparisons, as it is a distinct approach to creative generation.
        </p>


        
        <img style="width: 80%;" src="./resources/compare_mix.png" alt="Teaser figure."/>


        
    <br> <br>
    <hr>
    
      <h1>Ablation Study</h1>
          <p style="text-align: center">
            Ablation study of optimizing ϵt, injection step i and scale
            factor α from the third column to the fifth column.
          </p>


    <img style="width: 80%;" src="./resources/ablation.png" alt="Teaser figure."/>


    <br> <br>
    <hr>
    <h1>More Results</h1>

    <h2>Fused Results With three Prompts</h2>

      <p style="TEXT-ALIGN: justify">
        In this subsection, we
        demonstrate our results with multiple prompts, highlighting the potential of our ATIH model for continuous
        editing. 
    </p>
      <img style="width: 80%;" src="./resources/three_fuse.png" alt="Teaser figure."/>

    <h2>Comparison with complex prompt</h2>
      <p style="TEXT-ALIGN: justify">
        In this subsection, we compare our results with those of the DALLE·3 model assisted by Copilot. We used
        complex descriptive prompts for our editing results and provided the original images as input. The DALLE·3
        model then edited the original images using these complex prompts. Firstly, we observe that the DALLE·3
        model does not achieve results as acceptable as ours. Secondly, even with sufficient prompts, the DALLE·3
        model fails to maintain the original structure and layout of the images
    </p>
      <img style="width: 80%;" src="./resources/compex.png" alt="Teaser figure."/>

    <h2>More visual Results</h2>
      <p style="TEXT-ALIGN: justify">
        In this subsection, we present additional results of our model. we shows more generation results of our
        ATIH model. We used 4 images and edited them with 4 different text prompts.
    </p>
      <img style="width: 80%;" src="./resources/more_res.png" alt="Teaser figure."/>

<!--     <br><br>
    <hr>
    


    <h2>References</h2>
    <blockquote>
        <p id="ref1">[1]Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-
            conditional image generation with clip latents. In arXiv:2204.06125, 2022.</p>
        <p id="ref2">[2]Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj ̈orn Ommer. High-
            resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF
            Conference on Computer Vision and Pattern Recognition (CVPR), pp. 10684–10695, 2022.</p>
        <p id="ref3">[3]Zhida Feng, Zhenyu Zhang, Xintong Yu, Yewei Fang, Lanxin Li, Xuyi Chen, Yuxiang Lu, Ji-
            axiang Liu, Weichong Yin, Shikun Feng, Yu Sun, Hao Tian, Hua Wu, and Haifeng Wang.
            Ernie-vilg 2.0: Improving text-to-image diffusion model with knowledge-enhanced mixture-of-
            denoising-experts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
            Recognition (CVPR), pp. 10135–10145.2023.</p>
        <p id="ref4">[4]Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy.
            Pick-a-pic: An open dataset of user preferences for text-to-image generation. arXiv preprint
            arXiv:2305.01569, 2023.</p>    
        <p id="ref5">[5]Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li.
            Human preference score v2: A solid benchmark for evaluating human preferences of text-to-image
            synthesis. arXiv preprint arXiv:2306.09341, 2023.</p>        
        <p id="ref6">[6]Schuhmann C, Beaumont R, Vencu R, et al. Laion-5b: An open large-scale dataset for training next generation image-text models[J]. Advances in Neural Information Processing Systems, 2022, 35: 25278-25294.</p>
        <p id="ref7">[7]https://www.instagram.com/les.creatonautes/</p>
      </blockquote>
    
    <br>
    <hr> -->


<script>
    let slideIndex = 1;
    showSlides(slideIndex);
    
    function plusSlides(n) {
      showSlides(slideIndex += n);
    }
    
    function currentSlide(n) {
      showSlides(slideIndex = n);
    }
    
    function showSlides(n) {
      let i;
      let slides = document.getElementsByClassName("mySlides");
      let dots = document.getElementsByClassName("dot");
      if (n > slides.length) {slideIndex = 1}    
      if (n < 1) {slideIndex = slides.length}
      for (i = 0; i < slides.length; i++) {
        slides[i].style.display = "none";  
      }
      for (i = 0; i < dots.length; i++) {
        dots[i].className = dots[i].className.replace(" active", "");
      }
      slides[slideIndex-1].style.display = "block";  
      dots[slideIndex-1].className += " active";
    }
</script>

</body>

</html>
